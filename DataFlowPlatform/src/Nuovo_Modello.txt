Input files: 
    JSON containing the definition of a sequence of one or more maps operation and one reduce operation. All operations are referd to as a task.
    CSV file containing the input as a series of key-value pairs.
 
Coordinator
Setup:
    The coordinator has an array workersData, containing informations about the id, task in execution and state of each worker.
    Initially the Coordinator read and validates the JSON and the CSV files, the pairs are divided into chunks. 
    An auxiliary queue currentTaskQueue is used at each step to keep track of which operation needs to be scheduled next and on which chunks. At initialization it is filled with pairs containing the index of the first operation and the index of each chunk. We also keep an array of binary chunkDone representing which chunks we have already completed for the current task.
    The coordinator sends a ping to all workers and schedules a pingTimeout.

Runtime:
    The coordinator pops a task from the currentTaskQueue and assigns it to a free worker, assignment is performed by sending an executeTask message to such worker, after that the workersData structure is updated.
    When a taskCompletion message is recieved workersData and chunkDone is updated, another task from the queue is scheduled on that worker. 
    If the currentTaskQueue is emptied and chunkDone is all 1s (binary true) we start to execute the next operations. In doing so we reset chunkDone to all 0 and fill the queue with the next task.

    The coordinator will periodically ping all workers by sending a ping and scheduling a self pingTimeout message, if no pong is recieved before a pingTimeout for that worker then it is considered failed and its worker data is updated. When a worker that was executing a task fails, the task is put on top of the currentTaskQueue.
    If a pong is recieved in time the pingTimeout for that worker is deleted and a new pong is sent after some time, a new timeout is scheduled. 
    When a back online message is recieved the info about the worker is updated and a new task is scheduled.

    In order to more efficiently perform the reduce, we use a data structure that indexes pairs by [key mod numberOfChunks]. It is the job of the coordinator to organiza the reduceData array so that each cell contains values that belong to a single reduce operation.
    At the end of the reduce step the reduceData will contain the final output.   

Worker
Runtime:
    The worker is just a simple parser. It will recieve an executeTask message, parse the type of operation, read the data chunk and execute the corresponding operation. The output is then sent back. To simulate execution time the worker schedules an executionTime self message after which it will send a taskCompleted message.
    The worker also responds to each ping with a pong. The failure model is such that there is a probability to fail after each executeTask is revieced, in this case the worker will drop the task and schedule a recovery self message, after which it will send a backOnline to the coordinator.

